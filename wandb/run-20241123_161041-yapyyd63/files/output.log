Using device: cuda
Episode: 16, Reward: -15.6875, Advantage: -1.519819868661622e-16, Logits: 0.49609375, Probabilites: -0.701171875, Loss: -0.00000005, Time: 18.88s, Total Time: 0.31m
[34m[1mwandb[0m: [33mWARNING[0m Tried to log to step 0 that is less than the current step 16. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
Episode: 32, Reward: -16.6875, Advantage: -2.004868762915331e-16, Logits: 0.49609375, Probabilites: -0.701171875, Loss: -0.00000014, Time: 21.74s, Total Time: 0.68m
Traceback (most recent call last):
  File "C:\Users\Elene2004\Python\Files\PingPong-PPO\PG.py", line 112, in <module>
    observations, rewards, terminateds, truncateds, _ = envs.step(actions)
  File "C:\Users\Elene2004\anaconda3\envs\pong_rl\lib\site-packages\gymnasium\vector\sync_vector_env.py", line 222, in step
    ) = self.envs[i].step(action)
  File "C:\Users\Elene2004\anaconda3\envs\pong_rl\lib\site-packages\gymnasium\wrappers\common.py", line 393, in step
    return super().step(action)
  File "C:\Users\Elene2004\anaconda3\envs\pong_rl\lib\site-packages\gymnasium\core.py", line 322, in step
    return self.env.step(action)
  File "C:\Users\Elene2004\anaconda3\envs\pong_rl\lib\site-packages\gymnasium\wrappers\common.py", line 285, in step
    return self.env.step(action)
  File "C:\Users\Elene2004\anaconda3\envs\pong_rl\lib\site-packages\ale_py\env.py", line 299, in step
    reward += self.ale.act(action_idx, strength)
KeyboardInterrupt
